{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd2c0d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import threading\n",
    "import datetime\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from contextlib import redirect_stdout\n",
    "import re\n",
    "sys.path.append('../')\n",
    "from model_structure import create_my_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08f1d50",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ad3ed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pygame\n",
    "# this is config file:\n",
    "\n",
    "# Set the maximum amount of movement for the circle\n",
    "max_movement = 5\n",
    "\n",
    "#dimention of game window\n",
    "window_width, window_height = 600, 550\n",
    "\n",
    "# Set the color of the circle (in RGB format)\n",
    "circle_color = (255, 0, 0)  # Red\n",
    "\n",
    "#color of window background\n",
    "colorOfBackground = (0, 10, 0)\n",
    "\n",
    "# Set the color of the line (in RGB format)\n",
    "line_color = (255, 255, 255)  # blue\n",
    "border_line_color = (80, 50, 80)  # blue\n",
    "line_thick = 10\n",
    "\n",
    "# width of line\n",
    "width_of_lines = 100\n",
    "\n",
    "\n",
    "distance_from_window = 10\n",
    "\n",
    "line_start_x_left = distance_from_window\n",
    "line_end_x_left = distance_from_window\n",
    "line_start_x_right = window_width - distance_from_window\n",
    "line_end_x_right = window_width - distance_from_window\n",
    "\n",
    "radius = 30\n",
    "\n",
    "# Set the font for the exit button\n",
    "exit_button_text = \"Exit Game\"\n",
    "button_color = (255, 0, 0)\n",
    "exit_button_text_color = (255, 255, 255)\n",
    "exit_button_width = 100\n",
    "exit_button_height = 50\n",
    "exit_button_x = (window_width - exit_button_width) // 2\n",
    "exit_button_y = (window_height - exit_button_height) // 2 + 100\n",
    "\n",
    "# final score of game\n",
    "finalScore = 10000\n",
    "\n",
    "# backUp addres\n",
    "backUpFilePath = \"./modelBackUp.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb955503",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63d27b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check location of circle and if circle recive the edge of screen it allow to change direction of circle\n",
    "def circleDirection(center_x, center_y):\n",
    "    condition1 = (center_x > window_width - (radius+distance_from_window+line_thick))\n",
    "    condition2 = (center_x < (radius+distance_from_window+line_thick))\n",
    "    condition3 = ((center_y > window_height - (radius+distance_from_window+line_thick)))\n",
    "    condition4 = ((center_y < (radius+distance_from_window+line_thick)))\n",
    "    if (condition1 or condition2 or condition3 or condition4):\n",
    "        movement_flag = True\n",
    "        # here gameover checked\n",
    "        # Check location of circle and determine which side it has touched\n",
    "        if center_x <= radius + distance_from_window + line_thick:\n",
    "            # Circle touched the left side\n",
    "            return movement_flag, 'left'\n",
    "\n",
    "        elif center_x >= window_width - (radius + distance_from_window + line_thick):\n",
    "            # Circle touched the right side\n",
    "            return movement_flag, 'right'\n",
    "\n",
    "        elif center_y <= radius + distance_from_window + line_thick:\n",
    "            # Circle touched the top side\n",
    "            return movement_flag, 'top'\n",
    "\n",
    "        elif center_y >= window_height - (radius + distance_from_window + line_thick):\n",
    "            # Circle touched the bottom side\n",
    "            return movement_flag, 'bottom'\n",
    "    else:\n",
    "        return False, None #movement_flag, state\n",
    "\n",
    "def zeroCorr(num, state, epsilon):\n",
    "    if(num == 0):\n",
    "        if(state == -1):\n",
    "            return -epsilon\n",
    "        else:\n",
    "            return epsilon\n",
    "    else:\n",
    "        return num\n",
    "\n",
    "# Generate random movement for the circle\n",
    "def circleMovement(movement_fleg, center_x, center_y, p_center_x, p_center_y, edgeState):\n",
    "    counter = 1\n",
    "    epsilon = 0.01\n",
    "    if movement_fleg:\n",
    "        while True:\n",
    "            center_x_test = center_x\n",
    "            center_y_test = center_y\n",
    "            if(p_center_x < center_x and p_center_y > center_y):\n",
    "                if(edgeState == 'top'):\n",
    "                    movement_x = random.randint(1, max_movement)\n",
    "                    movement_y = random.randint(1, max_movement)\n",
    "                else:#right\n",
    "                    movement_x = random.randint(-max_movement, -1)\n",
    "                    movement_y = random.randint(-max_movement, -1)\n",
    "            elif(p_center_x > center_x and p_center_y > center_y):\n",
    "                if(edgeState == \"top\"):\n",
    "                    movement_x = random.randint(-max_movement, -1)\n",
    "                    movement_y = random.randint(1, max_movement)\n",
    "                else:#left\n",
    "                    movement_x = random.randint(1, max_movement)\n",
    "                    movement_y = random.randint(-max_movement, -1)\n",
    "            elif(p_center_x > center_x and p_center_y < center_y):\n",
    "                if(edgeState == 'left'):\n",
    "                    movement_x = random.randint(1, max_movement)\n",
    "                    movement_y = random.randint(1, max_movement)\n",
    "                else:#bottom\n",
    "                    movement_x = random.randint(-max_movement, -1)\n",
    "                    movement_y = random.randint(-max_movement, -1)\n",
    "            elif(p_center_x < center_x and p_center_y < center_y):\n",
    "                if(edgeState == 'right'):\n",
    "                    movement_x = random.randint(-max_movement, -1)\n",
    "                    movement_y = random.randint(1, max_movement)\n",
    "                else: #bottom\n",
    "                    movement_x = random.randint(1, max_movement)\n",
    "                    movement_y = random.randint(-max_movement, -1)\n",
    "            else:\n",
    "                movement_x = random.randint(-max_movement, max_movement)\n",
    "                movement_y = random.randint(-max_movement, max_movement)\n",
    "\n",
    "            if (counter > 10):\n",
    "                center_x_test = center_x\n",
    "                center_y_test = center_y\n",
    "                if(edgeState == 'top'):\n",
    "                    # if(p_center_x < center_x and p_center_y > center_y):\n",
    "                    if(random.randint(0, 1)):\n",
    "                        movement_x = 1\n",
    "                        movement_y = 1\n",
    "                    else:\n",
    "                        movement_x = -1\n",
    "                        movement_y = 1\n",
    "                if(edgeState == 'right'):\n",
    "                    # if(p_center_x < center_x and p_center_y < center_y):\n",
    "                    if(random.randint(0, 1)):\n",
    "                        movement_x = -1\n",
    "                        movement_y = 1\n",
    "                    else:\n",
    "                        movement_x = -1\n",
    "                        movement_y = -1\n",
    "                if(edgeState == 'bottom'):\n",
    "                    # if(p_center_x > center_x and p_center_y < center_y):\n",
    "                    if(random.randint(0, 1)):\n",
    "                        movement_x = 1\n",
    "                        movement_y = -1\n",
    "                    else:\n",
    "                        movement_x = -1\n",
    "                        movement_y = -1\n",
    "                if(edgeState == 'left'):\n",
    "                    # if(p_center_x > center_x and p_center_y > center_y):\n",
    "                    if(random.randint(0, 1)):\n",
    "                        movement_x = 1\n",
    "                        movement_y = -1\n",
    "                    else:\n",
    "                        movement_x = 1\n",
    "                        movement_y = 1\n",
    "                break\n",
    "\n",
    "            center_x_test += movement_x\n",
    "            center_y_test += movement_y\n",
    "            # check next location of circle and if the next locatio is out of screen determine new movement\n",
    "            condition1 = (center_x_test <= window_width-(radius+distance_from_window+line_thick))\n",
    "            condition2 = (center_x_test >= (radius+distance_from_window+line_thick))\n",
    "            condition3 = (center_y_test <= window_height-(radius+distance_from_window+line_thick))\n",
    "            condition4 = (center_y_test >= (radius+distance_from_window+line_thick))\n",
    "            # print(counter)\n",
    "            if(condition1 and condition2 and condition3 and condition4):\n",
    "                break\n",
    "            counter +=1\n",
    "        movement_fleg = False\n",
    "        return movement_fleg, movement_x, movement_y\n",
    "\n",
    "def referee(mouse_y, center_y, y_opponent, edgeState):\n",
    "    quitGame = False\n",
    "    if(edgeState == 'left'):\n",
    "        if(mouse_y-(width_of_lines/2)<center_y<mouse_y+(width_of_lines/2)):\n",
    "            quitGame = False\n",
    "            return quitGame\n",
    "        else:\n",
    "            quitGame = True\n",
    "            return quitGame\n",
    "    elif(edgeState == 'right'):\n",
    "        if (y_opponent - (width_of_lines / 2) < center_y < y_opponent + (width_of_lines / 2)):\n",
    "            quitGame = False\n",
    "            return quitGame\n",
    "        else:\n",
    "            quitGame = True\n",
    "            return quitGame\n",
    "    else:\n",
    "        quitGame = False\n",
    "        return quitGame\n",
    "\n",
    "\n",
    "def update_scores(player1_score, player2_score):\n",
    "    pass\n",
    "\n",
    "\n",
    "# reward function\n",
    "def opponentReward(opponentMovement, y_opponent, circle_y, circleNotStrike, edgeState, faultForMOveToCourner):\n",
    "    reward = 0 #if controller not moved\n",
    "    if(opponentMovement != 0):\n",
    "        reward = -0.5\n",
    "    if(faultForMOveToCourner == True):\n",
    "        reward = -10\n",
    "    if(edgeState == 'right'):\n",
    "        a = 500\n",
    "        c = 59.2\n",
    "        if(circleNotStrike == True):\n",
    "            reward = (a*np.exp(-((abs(y_opponent-circle_y))**2)/(2*(c**2))))-350\n",
    "        else:\n",
    "            reward = ((a * np.exp(-((abs(y_opponent-circle_y))**2) / (2 * (c**2)))) - 350) * 2.3\n",
    "    # print(\"reward is: \", reward)\n",
    "    return float(reward)\n",
    "\n",
    "# log functions\n",
    "def log_to_json(log_data, log_filename='log.json'):\n",
    "    # Check if the log file already exists\n",
    "    if os.path.exists(log_filename):\n",
    "        # If it exists, load the existing log data\n",
    "        with open(log_filename, 'r') as file:\n",
    "            try:\n",
    "                existing_data = json.load(file)\n",
    "            except json.JSONDecodeError:\n",
    "                existing_data = []\n",
    "\n",
    "        # Append the new log data to the existing data\n",
    "        existing_data.append(log_data)\n",
    "    else:\n",
    "        # If the log file doesn't exist, create a new log data list\n",
    "        existing_data = [log_data]\n",
    "\n",
    "    # Write the combined log data back to the log file\n",
    "    with open(log_filename, 'w') as file:\n",
    "        json.dump(existing_data, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3b6f35",
   "metadata": {},
   "source": [
    "# QValueFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68b97fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValueFunction:\n",
    "    def __init__(self, state_size, action_size, x_min, x_max, initial_learning_rate = 0.01):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.model = self.build_model(initial_learning_rate)\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "\n",
    "    def build_model(self, initial_learning_rate):\n",
    "        model = create_my_model(self.state_size, self.action_size)\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "        return model\n",
    "\n",
    "    def compute_accuracy(self, inputs, targets):\n",
    "        predicted_probabilities = np.abs(self.model.predict(inputs).reshape(-1))\n",
    "        targets = np.abs(targets)\n",
    "        \n",
    "        max_predicted = np.max(predicted_probabilities)\n",
    "        max_targets = np.max(targets)\n",
    "\n",
    "\n",
    "        if max_predicted == 0 or max_targets == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        normalized_predicted = predicted_probabilities / max_predicted\n",
    "        normalized_targets = targets / max_targets\n",
    "        if(np.max(normalized_predicted) > 1 or np.max(normalized_targets) > 1 or np.max(normalized_predicted) < -1 or np.max(normalized_targets) < -1):\n",
    "            print(\"normalized_predicted\", np.max(normalized_predicted), \"normalized_targets\", np.max(normalized_targets))\n",
    "        \n",
    "        accuracy = np.mean(np.abs(normalized_predicted - normalized_targets))\n",
    "        return (1 - accuracy)\n",
    "\n",
    "\n",
    "    def train(self, states, actions, targets, numberOfTrain):\n",
    "        maxEpock = numberOfTrain\n",
    "        sample_step = 10\n",
    "        sample_print = 500\n",
    "        loss_value = float('inf')  # Initialize the loss value to a large number\n",
    "        epochs = 0\n",
    "        loss_history = []\n",
    "        accuracy_history = []\n",
    "        accuracy_val = 0\n",
    "        accuracy_threshold = 0.9\n",
    "        inputs = np.hstack((states, np.expand_dims(actions, axis=-1)))  # Concatenate states and actions\n",
    "        print(\"number of data is: \", inputs.shape[0])\n",
    "        \n",
    "        if(self.compute_accuracy(inputs, targets) > accuracy_threshold):\n",
    "            print(\"accuracy is upper than: \", accuracy_threshold, \"it is: \", self.compute_accuracy(inputs, targets))\n",
    "            return 0\n",
    "        \n",
    "        while 1:\n",
    "            history = self.model.fit(inputs, targets, epochs=1, verbose=0, batch_size=int(inputs.shape[0]/2))\n",
    "            epochs += 1\n",
    "            if(epochs%sample_step == 0):\n",
    "              loss_value = history.history['loss'][-1]\n",
    "              loss_history.append(loss_value)\n",
    "              accuracy_val = self.compute_accuracy(inputs, targets)\n",
    "              accuracy_history.append(accuracy_val)\n",
    "              if(epochs%sample_print == 0):\n",
    "                print(\"accuracy: \", accuracy_val)\n",
    "                print(f\"Epoch: {epochs}, Loss value: {loss_value}\")\n",
    "\n",
    "            if ((epochs > maxEpock) or (accuracy_val > accuracy_threshold)):\n",
    "              print(\"accuracy: \", accuracy_val)\n",
    "              print(f\"Epoch: {epochs}, Loss value: {loss_value}\")\n",
    "              break\n",
    "\n",
    "        # Create a figure with two subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        ax1.plot(range(1, len(loss_history) + 1), loss_history)\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss Value')\n",
    "        ax1.set_title('Training Loss History')\n",
    "        ax1.grid(True)\n",
    "\n",
    "        ax2.plot(range(1, len(accuracy_history) + 1), accuracy_history)\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.set_title('Training Accuracy History')\n",
    "        ax2.grid(True)\n",
    "\n",
    "        # Create a directory for saving the plots if it doesn't exist\n",
    "        save_dir = './loss_plots/'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        # Generate a unique filename using the current timestamp\n",
    "        timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "        file_name = f'loss_curve_{timestamp}.png'\n",
    "        # Save the plot to the specific directory with the unique filename\n",
    "        plt.savefig(os.path.join(save_dir, file_name))\n",
    "        # Close the plot to free up memory\n",
    "        plt.close()\n",
    "\n",
    "    def predict(self, states):\n",
    "        num_actions = self.x_max - self.x_min + 1\n",
    "        action_values = np.arange(self.x_min, self.x_max + 1)\n",
    "        states = np.array(states)\n",
    "        num_states = states.shape[0]\n",
    "        states = np.hstack((states, np.zeros((num_states, 1), dtype=int)))  # Add a column with zeros\n",
    "        # Create a state-action matrix with all possible actions for each state\n",
    "        state_plus_actions = np.repeat(states, num_actions, axis=0)\n",
    "        action_tile = np.tile(action_values, num_states)\n",
    "        state_plus_actions[:, -1] = action_tile\n",
    "        # Predict Q-values for all state-action pairs in parallel\n",
    "        q_values = self.model.predict(state_plus_actions)\n",
    "        # Reshape the Q-values to have a separate row for each state\n",
    "        q_values = q_values.reshape(num_states, num_actions)\n",
    "        # Find the action corresponding to the minimum Q-value for each state\n",
    "        min_q_values = np.max(q_values, axis=1)\n",
    "        return min_q_values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5d257b",
   "metadata": {},
   "source": [
    "# RLAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77792d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design the RL Agent\n",
    "class RLAgent:\n",
    "    def __init__(self, state_size, action_size, x_min, x_max, discount_factor,  buffer_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.discount_factor = discount_factor\n",
    "        self.model = self.build_model()\n",
    "        self.backup_model = None  # Initialize the backup model as None\n",
    "\n",
    "    def build_model(self):\n",
    "        model = create_my_model(self.state_size, self.action_size)\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "        return model\n",
    "\n",
    "    def get_action(self, circle_x, circle_y, control_line_y):\n",
    "        threshold = 0.05\n",
    "        random_number = random.random()  # Generate a random number between 0 and 1\n",
    "        action_values = np.arange(self.x_min, self.x_max + 1)\n",
    "        if random_number > threshold:\n",
    "            num_actions = self.x_max - self.x_min + 1\n",
    "            state = np.array([[circle_x, circle_y, control_line_y, 0]])\n",
    "            # Create a state-action matrix with all possible actions\n",
    "            state_plus_actions = np.tile(state, (num_actions, 1))\n",
    "            state_plus_actions[:, -1] = action_values\n",
    "            # Predict Q-values for all state-action pairs in parallel\n",
    "            q_values = self.model.predict(state_plus_actions, verbose= -1)\n",
    "            # Find the action corresponding to the minimum Q-value\n",
    "            action = action_values[np.argmax(q_values)]\n",
    "            # print(np.max(q_values))\n",
    "        else:\n",
    "            action = random.choice(action_values)\n",
    "        return action\n",
    "\n",
    "    def add_experience(self, state, action, reward, next_state):\n",
    "        experience = (state, action, reward, next_state)\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def free_up_buffer(self):\n",
    "        # Reassign the buffer to a new empty deque\n",
    "        self.buffer = deque(maxlen=self.buffer_size)\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states = np.array([e[0] for e in batch])\n",
    "        actions = np.array([e[1] for e in batch])\n",
    "        rewards = np.array([e[2] for e in batch])\n",
    "        next_states = np.array([e[3] for e in batch])\n",
    "        return states, actions, rewards, next_states\n",
    "\n",
    "    def get_full_batch(self):\n",
    "        states = np.array([e[0] for e in self.buffer])\n",
    "        actions = np.array([e[1] for e in self.buffer])\n",
    "        rewards = np.array([e[2] for e in self.buffer])\n",
    "        next_states = np.array([e[3] for e in self.buffer])\n",
    "        return states, actions, rewards, next_states\n",
    "\n",
    "    def save_backup_model(self, filepath):\n",
    "        if self.backup_model is not None:\n",
    "            self.backup_model.save_weights(filepath)\n",
    "\n",
    "    def load_backup_model(self, filepath):\n",
    "        # Create a new QValueFunction model\n",
    "        q_value_model = QValueFunction(self.state_size, self.action_size, self.x_min, self.x_max)\n",
    "        # Load the model weights from the file\n",
    "        q_value_model.model.load_weights(filepath)\n",
    "        self.model = q_value_model.model\n",
    "        # Set the loaded model as the backup model\n",
    "        self.backup_model = q_value_model.model\n",
    "        q_value_model = None\n",
    "\n",
    "    def train_q_value_function(self, numberOfTrain, initial_learning_rate):\n",
    "        q_value_model = QValueFunction(self.state_size, self.action_size, self.x_min, self.x_max, initial_learning_rate)\n",
    "        q_value_model.model = self.model\n",
    "        # Sample a batch from the replay buffer\n",
    "        states, actions, rewards, next_states = self.get_full_batch()\n",
    "        # Compute the target Q-values using the Bellman equation\n",
    "        target_q_values = rewards + self.discount_factor * q_value_model.predict(next_states)\n",
    "        # Train the QValueFunction model using the batch\n",
    "        q_value_model.train(states, actions, target_q_values, numberOfTrain)\n",
    "        self.model = q_value_model.model\n",
    "        self.backup_model = q_value_model.model  # Update the backup model with the trained model\n",
    "        self.save_backup_model(backUpFilePath)\n",
    "        q_value_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316ffa38",
   "metadata": {},
   "source": [
    "# important variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96155ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numTrain bufferSize discountFactor numberOfTest TestThreshold initLearningRate\n",
    "# 7000, 16000, 0.95, 40, 30, 0.01 --> accuracy: 0.5\n",
    "# 500, 2000, 0.95, 5, 5, 0.01  --> accuracy: 0.7\n",
    "# 1000, 4000, 0.95, 10, 8, 0.01 --> accuracy: 0.1\n",
    "\n",
    "numberOftrain = 500\n",
    "buffer_size = 2500  # Set your preferred buffer size\n",
    "discount_factor = 0.96\n",
    "number_of_test_mode_cycle = 20\n",
    "number_of_ture_prediction_thereshold = 15\n",
    "initial_learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879d8fbf",
   "metadata": {},
   "source": [
    "# train mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7f12f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regular mode: \n",
      "experiment sample:  1000\n",
      "regular mode: \n",
      "experiment sample:  2000\n",
      "ball cycle =  8\n",
      "train model begin\n",
      "number of data is:  2500\n",
      "accuracy:  0.5845435418249776\n",
      "Epoch: 500, Loss value: 219.6535186767578\n",
      "accuracy:  0.5845435418249776\n",
      "Epoch: 501, Loss value: 219.6535186767578\n",
      "test mode runing...\n",
      "test mode: \n",
      "Ball Cycle:  0\n",
      "test mode: \n",
      "Ball Cycle:  10\n",
      "test mode: \n",
      "Ball Cycle:  20\n",
      "**** new best in reward ****\n",
      "**** new best in prediction ****\n",
      "*******************************\n",
      "*                             *\n",
      "rate of true prediction:  0.4\n",
      "sum of reward:  -15584.104545148279\n",
      "*                             *\n",
      "*******************************\n",
      "test mode ended\n",
      "regular mode: \n",
      "experiment sample:  0\n",
      "regular mode: \n",
      "experiment sample:  1000\n",
      "regular mode: \n",
      "experiment sample:  2000\n",
      "ball cycle =  6\n",
      "train model begin\n",
      "number of data is:  2500\n",
      "accuracy:  0.8599192355816561\n",
      "Epoch: 500, Loss value: 317.6688537597656\n",
      "accuracy:  0.8599192355816561\n",
      "Epoch: 501, Loss value: 317.6688537597656\n",
      "test mode runing...\n",
      "test mode: \n",
      "Ball Cycle:  0\n",
      "test mode: \n",
      "Ball Cycle:  10\n",
      "test mode: \n",
      "Ball Cycle:  20\n",
      "**** new best in reward ****\n",
      "*******************************\n",
      "*                             *\n",
      "rate of true prediction:  0.25\n",
      "sum of reward:  -9204.47059085561\n",
      "*                             *\n",
      "*******************************\n",
      "test mode ended\n",
      "regular mode: \n",
      "experiment sample:  0\n",
      "regular mode: \n",
      "experiment sample:  1000\n",
      "regular mode: \n",
      "experiment sample:  2000\n",
      "ball cycle =  7\n",
      "train model begin\n",
      "number of data is:  2500\n",
      "accuracy:  0.8719095053606721\n",
      "Epoch: 500, Loss value: 301.56060791015625\n",
      "accuracy:  0.8719095053606721\n",
      "Epoch: 501, Loss value: 301.56060791015625\n",
      "test mode runing...\n",
      "test mode: \n",
      "Ball Cycle:  0\n",
      "test mode: \n",
      "Ball Cycle:  10\n",
      "test mode: \n",
      "Ball Cycle:  20\n",
      "*******************************\n",
      "*                             *\n",
      "rate of true prediction:  0.1\n",
      "sum of reward:  -17534.725114112407\n",
      "*                             *\n",
      "*******************************\n",
      "test mode ended\n",
      "regular mode: \n",
      "experiment sample:  0\n",
      "regular mode: \n",
      "experiment sample:  1000\n",
      "regular mode: \n",
      "experiment sample:  2000\n",
      "ball cycle =  8\n",
      "train model begin\n",
      "number of data is:  2500\n",
      "accuracy:  0.9016870666528634\n",
      "Epoch: 110, Loss value: 298.42041015625\n",
      "test mode runing...\n",
      "test mode: \n",
      "Ball Cycle:  0\n",
      "test mode: \n",
      "Ball Cycle:  10\n",
      "test mode: \n",
      "Ball Cycle:  20\n",
      "*******************************\n",
      "*                             *\n",
      "rate of true prediction:  0.3\n",
      "sum of reward:  -11578.237723597485\n",
      "*                             *\n",
      "*******************************\n",
      "test mode ended\n",
      "regular mode: \n",
      "experiment sample:  0\n",
      "regular mode: \n",
      "experiment sample:  1000\n",
      "regular mode: \n",
      "experiment sample:  2000\n",
      "ball cycle =  6\n",
      "train model begin\n",
      "number of data is:  2500\n",
      "accuracy is upper than:  0.9 it is:  0.9019095107656261\n",
      "test mode runing...\n",
      "test mode: \n",
      "Ball Cycle:  0\n",
      "test mode: \n",
      "Ball Cycle:  10\n",
      "test mode: \n",
      "Ball Cycle:  20\n",
      "**** new best in prediction ****\n",
      "*******************************\n",
      "*                             *\n",
      "rate of true prediction:  0.45\n",
      "sum of reward:  -10462.538839103412\n",
      "*                             *\n",
      "*******************************\n",
      "test mode ended\n",
      "regular mode: \n",
      "experiment sample:  0\n",
      "regular mode: \n",
      "experiment sample:  1000\n",
      "regular mode: \n",
      "experiment sample:  2000\n",
      "ball cycle =  6\n",
      "train model begin\n",
      "number of data is:  2500\n",
      "accuracy:  0.8711576743086358\n",
      "Epoch: 500, Loss value: 318.51776123046875\n",
      "accuracy:  0.8711576743086358\n",
      "Epoch: 501, Loss value: 318.51776123046875\n",
      "test mode runing...\n",
      "test mode: \n",
      "Ball Cycle:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x000001996F30D9D0>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 536, in __del__\n",
      "    gen_dataset_ops.delete_iterator(\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\", line 1276, in delete_iterator\n",
      "    _result = pywrap_tfe.TFE_Py_FastPathExecute(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mode: \n",
      "Ball Cycle:  10\n",
      "test mode: \n",
      "Ball Cycle:  20\n",
      "**** new best in reward ****\n",
      "*******************************\n",
      "*                             *\n",
      "rate of true prediction:  0.3\n",
      "sum of reward:  -8695.99621545281\n",
      "*                             *\n",
      "*******************************\n",
      "test mode ended\n",
      "regular mode: \n",
      "experiment sample:  0\n",
      "regular mode: \n",
      "experiment sample:  1000\n",
      "regular mode: \n",
      "experiment sample:  2000\n",
      "ball cycle =  6\n",
      "train model begin\n",
      "number of data is:  2500\n",
      "accuracy:  0.8732196287009975\n",
      "Epoch: 500, Loss value: 259.23333740234375\n",
      "accuracy:  0.8732196287009975\n",
      "Epoch: 501, Loss value: 259.23333740234375\n",
      "test mode runing...\n",
      "test mode: \n",
      "Ball Cycle:  0\n",
      "test mode: \n",
      "Ball Cycle:  10\n",
      "test mode: \n",
      "Ball Cycle:  20\n",
      "**** new best in reward ****\n",
      "*******************************\n",
      "*                             *\n",
      "rate of true prediction:  0.2\n",
      "sum of reward:  -6562.513727174651\n",
      "*                             *\n",
      "*******************************\n",
      "test mode ended\n",
      "regular mode: \n",
      "experiment sample:  0\n",
      "regular mode: \n",
      "experiment sample:  1000\n",
      "regular mode: \n",
      "experiment sample:  2000\n",
      "ball cycle =  6\n",
      "train model begin\n",
      "number of data is:  2500\n",
      "accuracy:  0.8665799743095619\n",
      "Epoch: 500, Loss value: 100.73969268798828\n",
      "accuracy:  0.8665799743095619\n",
      "Epoch: 501, Loss value: 100.73969268798828\n",
      "test mode runing...\n",
      "test mode: \n",
      "Ball Cycle:  0\n",
      "test mode: \n",
      "Ball Cycle:  10\n",
      "test mode: \n",
      "Ball Cycle:  20\n",
      "**** new best in reward ****\n",
      "*******************************\n",
      "*                             *\n",
      "rate of true prediction:  0.3\n",
      "sum of reward:  -6327.210979053903\n",
      "*                             *\n",
      "*******************************\n",
      "test mode ended\n",
      "regular mode: \n",
      "experiment sample:  0\n",
      "regular mode: \n",
      "experiment sample:  1000\n",
      "regular mode: \n",
      "experiment sample:  2000\n",
      "ball cycle =  5\n",
      "train model begin\n",
      "number of data is:  2500\n",
      "accuracy:  0.877873943535751\n",
      "Epoch: 500, Loss value: 185.10740661621094\n",
      "accuracy:  0.877873943535751\n",
      "Epoch: 501, Loss value: 185.10740661621094\n",
      "test mode runing...\n",
      "test mode: \n",
      "Ball Cycle:  0\n",
      "test mode: \n",
      "Ball Cycle:  10\n"
     ]
    }
   ],
   "source": [
    "# Set the center coordinates\n",
    "center_x, center_y = window_width // 2, window_height // 2\n",
    "p_center_x = center_x\n",
    "p_center_y = center_y\n",
    "# y position of opponent plate in game\n",
    "y_opponent = center_y\n",
    "# define postoin of circle in window\n",
    "edgeState = None\n",
    "# delay of end of every frame to slow down game\n",
    "\n",
    "# if this flag set to True mean that the direction of circle should be changed\n",
    "movement_fleg = False\n",
    "movement_x = 1\n",
    "movement_y = 1\n",
    "faultForMOveToCourner = False\n",
    "# Main game loop\n",
    "running = True\n",
    "\n",
    "# score of players in the game\n",
    "player1_score = 0\n",
    "player2_score = 0\n",
    "\n",
    "# control line parameter\n",
    "# Define the RL Environment\n",
    "state_size = 3  # Assuming the state is represented by center_y and y_opponent\n",
    "action_size = 1  # Assuming the action is a single continuous value\n",
    "action_min_value = -4\n",
    "action_max_value = 4\n",
    "# buffer_size = 2000  # Set your preferred buffer size\n",
    "# discount_factor = 0.95\n",
    "opponentRl = RLAgent(state_size, action_size, action_min_value, action_max_value, discount_factor, buffer_size)\n",
    "\n",
    "# model log\n",
    "\n",
    "log_entry = {\n",
    "    'time': str(time.time()),\n",
    "    'number of epoch': str(numberOftrain),\n",
    "    'buffer size': str(buffer_size),\n",
    "    'discount factor': str(discount_factor),\n",
    "    'number of test sample': str(number_of_test_mode_cycle),\n",
    "    'initial learning rate': str(initial_learning_rate)\n",
    "}\n",
    "\n",
    "log_to_json(log_entry)\n",
    "\n",
    "# Capture the model summary output\n",
    "with open('model_summary.txt', 'w') as summary_file:\n",
    "    with redirect_stdout(summary_file):\n",
    "        opponentRl.model.summary()\n",
    "\n",
    "# Read the captured model summary and extract \"Output Shape\" information\n",
    "with open('model_summary.txt', 'r') as summary_file:\n",
    "    summary_text = summary_file.read()\n",
    "\n",
    "# Use regular expressions to extract \"(None, XX)\" patterns\n",
    "output_shape_info = re.findall(r'\\(None, \\d+\\)', summary_text)\n",
    "\n",
    "# Create a log entry with the extracted \"Output Shape\" information\n",
    "log_entry = {\n",
    "    'output_shape_info': output_shape_info\n",
    "}\n",
    "\n",
    "# Call the log_to_json function to append the log entry to the log file\n",
    "log_to_json(log_entry)\n",
    "\n",
    "# if recently model trained\n",
    "# if os.path.exists(backUpFilePath):\n",
    "#     opponentRl.load_backup_model(backUpFilePath)\n",
    "\n",
    "reward = 0\n",
    "\n",
    "howManyTimeLearnCyckeHappen = 1\n",
    "\n",
    "howManySampleInBuffer = buffer_size  # depend on buffer\n",
    "howManySampleInBuffer_counter = 0\n",
    "\n",
    "predictActionNumber = 4\n",
    "predictActionNumber_counter = 0\n",
    "y_opponent_movement = 0\n",
    "\n",
    "load_backUp_counter = 0\n",
    "load_backUp_threshold = 2\n",
    "\n",
    "number_of_true_prediction = 0\n",
    "number_of_true_prediction_old = 0\n",
    "# number_of_test_mode_cycle = 5\n",
    "# number_of_ture_prediction_thereshold = 5\n",
    "testMode = False\n",
    "\n",
    "\n",
    "sum_of_reward = 0\n",
    "sum_of_reward_best = -100000\n",
    "\n",
    "initial_learning_rate_counter = 0\n",
    "initial_learning_rate_threshold = 5\n",
    "\n",
    "reward_history = []\n",
    "\n",
    "\n",
    "while running:\n",
    "    # Generate random movement for the circle\n",
    "    if movement_fleg:\n",
    "        movement_fleg, movement_x, movement_y = circleMovement(movement_fleg, center_x, center_y, p_center_x, p_center_y, edgeState)\n",
    "\n",
    "    # Update the center coordinates of the circle\n",
    "    p_center_x = center_x\n",
    "    p_center_y = center_y\n",
    "    center_x += movement_x\n",
    "    center_y += movement_y\n",
    "\n",
    "    # check location of circle and if circle recive the edge of screen it allow to change direction of circle\n",
    "    movement_fleg, edgeState = circleDirection(center_x, center_y)\n",
    "\n",
    "    mouse_y = 0  # this value just for referee function\n",
    "\n",
    "    # opponent action\n",
    "    if predictActionNumber_counter == predictActionNumber:\n",
    "        y_opponent_movement = opponentRl.get_action(center_x, center_y, y_opponent)\n",
    "        predictActionNumber_counter = 0\n",
    "    predictActionNumber_counter += 1\n",
    "\n",
    "    y_old_state = y_opponent  # this variable used in add_experiment function of RLAgent class\n",
    "    if (((y_opponent + y_opponent_movement) <= (window_height - (width_of_lines / 2))) and ((y_opponent + y_opponent_movement) >= (width_of_lines / 2))):\n",
    "        y_opponent += y_opponent_movement\n",
    "        faultForMOveToCourner = False\n",
    "    else:\n",
    "        faultForMOveToCourner = True\n",
    "\n",
    "\n",
    "    # referee part\n",
    "    quitGame = referee(mouse_y, center_y, y_opponent, edgeState)\n",
    "\n",
    "    if (not quitGame):\n",
    "        if (edgeState == 'left'):\n",
    "            player1_score += 1\n",
    "        elif (edgeState == 'right'):\n",
    "            player2_score += 1\n",
    "            if (testMode):\n",
    "                number_of_true_prediction += 1\n",
    "\n",
    "    # reward function for take reward to action of opponent movement in game\n",
    "    reward = opponentReward(y_opponent_movement, y_opponent, center_y, quitGame, edgeState, faultForMOveToCourner)\n",
    "\n",
    "    if (not testMode):\n",
    "        # create train dataset to train model for y_controller of opponent player, (s, a, R(s), s')\n",
    "        opponentRl.add_experience((p_center_x, p_center_y, y_old_state), y_opponent_movement, reward, (center_x, center_y, y_opponent))\n",
    "        howManySampleInBuffer_counter += 1\n",
    "\n",
    "    if (not testMode):\n",
    "        if howManySampleInBuffer_counter == (howManySampleInBuffer):\n",
    "            print(\"ball cycle = \", howManyTimeLearnCyckeHappen)\n",
    "            howManyTimeLearnCyckeHappen = -1\n",
    "            howManySampleInBuffer_counter = 0\n",
    "            print(\"train model begin\")\n",
    "\n",
    "            if(initial_learning_rate_counter == initial_learning_rate_threshold):\n",
    "                initial_learning_rate /= 5\n",
    "                initial_learning_rate_counter = 0\n",
    "            initial_learning_rate_counter += 1\n",
    "\n",
    "            opponentRl.train_q_value_function(numberOftrain, initial_learning_rate)\n",
    "            opponentRl.free_up_buffer()\n",
    "            testMode = True\n",
    "            print(\"test mode runing...\")\n",
    "    # test mode:\n",
    "    else:\n",
    "        sum_of_reward += reward\n",
    "        if howManyTimeLearnCyckeHappen == (number_of_test_mode_cycle):\n",
    "            howManyTimeLearnCyckeHappen = -1\n",
    "            if (sum_of_reward > sum_of_reward_best):\n",
    "                opponentRl.save_backup_model(\"./modelBackUp_best_inReward.h5\")\n",
    "                sum_of_reward_best = sum_of_reward\n",
    "                load_backUp_counter = 0\n",
    "                print(\"**** new best in reward ****\")\n",
    "                log_to_json(\"new best in reward\")\n",
    "#             else:\n",
    "#                 if os.path.exists(\"./modelBackUp_best_inReward.h5\" and (load_backUp_counter < load_backUp_threshold)):\n",
    "#                     opponentRl.load_backup_model(\"./modelBackUp_best_inReward.h5\")\n",
    "#                     load_backUp_counter += 1\n",
    "\n",
    "            if (number_of_true_prediction > number_of_true_prediction_old):\n",
    "                opponentRl.save_backup_model(\"./modelBackUp_best_inPrediction.h5\")\n",
    "                number_of_true_prediction_old = number_of_true_prediction\n",
    "                log_to_json(\"**new_max_prediction**\")\n",
    "                print(\"**** new best in prediction ****\")\n",
    "            if (number_of_true_prediction >= number_of_ture_prediction_thereshold):\n",
    "                print(\"you have good model!\")\n",
    "                print(\"rate of true prediction: \", number_of_true_prediction/number_of_test_mode_cycle)\n",
    "                print(\"sum of reward: \", sum_of_reward)\n",
    "                log_to_json(\"rate: \"+str(number_of_true_prediction/number_of_test_mode_cycle))\n",
    "                log_to_json(\"reward: \"+str(sum_of_reward))\n",
    "                break\n",
    "            else:\n",
    "                testMode = False\n",
    "                howManyTimeLearnCyckeHappen = -1\n",
    "                print(\"*******************************\")\n",
    "                print(\"*                             *\")\n",
    "                print(\"rate of true prediction: \", number_of_true_prediction/number_of_test_mode_cycle)\n",
    "                print(\"sum of reward: \", sum_of_reward)\n",
    "                print(\"*                             *\")\n",
    "                print(\"*******************************\")\n",
    "                log_to_json(\"rate: \"+str(number_of_true_prediction/number_of_test_mode_cycle))\n",
    "                log_to_json(\"reward: \"+str(sum_of_reward))\n",
    "                log_to_json(\"------------\")\n",
    "                reward_history.append(sum_of_reward)\n",
    "                number_of_true_prediction = 0\n",
    "                sum_of_reward = 0\n",
    "                print(\"test mode ended\")\n",
    "\n",
    "    if (edgeState == 'right'):\n",
    "        howManyTimeLearnCyckeHappen += 1\n",
    "\n",
    "    if (testMode):\n",
    "        if (edgeState == 'right'):\n",
    "            if (howManyTimeLearnCyckeHappen % 10 == 0):\n",
    "                print(\"test mode: \")\n",
    "                print(\"Ball Cycle: \", howManyTimeLearnCyckeHappen)\n",
    "    else:\n",
    "        if (howManySampleInBuffer_counter % 1000 == 0):\n",
    "            print(\"regular mode: \")\n",
    "            print(\"experiment sample: \", howManySampleInBuffer_counter)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(reward_history, marker='o', linestyle='-')\n",
    "plt.title('Reward History')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Reward')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "save_dir = './reward_history/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "# Generate a unique filename using the current timestamp\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "file_name = f'reward_history_{timestamp}.png'\n",
    "# Save the plot to the specific directory with the unique filename\n",
    "plt.savefig(os.path.join(save_dir, file_name))\n",
    "# Close the plot to free up memory\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4eac54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
